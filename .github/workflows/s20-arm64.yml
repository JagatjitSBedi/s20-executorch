name: S20 ARM64 DeepSeek .pte
on:
  push:
    paths: ['.github/workflows/s20-arm64.yml']
  workflow_dispatch:
jobs:
  arm64:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: ARM64 PyTorch + DeepSeek Export
      uses: uraimo/run-on-arch-action@v2
      with:
        arch: aarch64
        distro: ubuntu22.04
        run: |
          apt-get update -qq
          apt-get install -y python3 python3-pip git
          pip3 install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip3 install --no-cache-dir executorch[export] transformers accelerate
          python3 -c "
import torch
print('PyTorch', torch.__version__)
import executorch
print('ExecuTorch OK')
from transformers import AutoTokenizer, AutoModelForCausalLM
from executorch.exir_server import ExirServer
model_id = 'deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct'
print('Loading', model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map='cpu', trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model.eval()
def generate(prompt):
    inputs = tokenizer(prompt, return_tensors='pt')
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=32)
    return tokenizer.decode(outputs[0])
model_example = tokenizer('Write Python code:', return_tensors='pt')
print('Exporting s20_deepseek.pte')
edge_exported_program = ExirServer.compile(generate, model_example)
edge_exported_program.exported_program().write_to_file('s20_deepseek.pte')
print('DeepSeek PTE ready!')
          "
    - name: Upload Artifact
      uses: actions/upload-artifact@v4
      with:
        name: s20_deepseek_pte
        path: s20_deepseek.pte
